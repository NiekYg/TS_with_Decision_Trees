{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas torch xgboost scikit-learn\n",
    "%pip install contextualbandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the Models folder to sys.path\n",
    "sys.path.append('/Users/niekvandergaag/Documents/Thesis/Code')\n",
    "\n",
    "\n",
    "# Now you can import from Models\n",
    "from Models.helperfunctions import *\n",
    "from Models.ETE import *\n",
    "from Models.TSnocontext import *\n",
    "from Datasets.Bibtex import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from Models.helperfunctions import *\n",
    "from Models.ETE import *\n",
    "from Models.TSnocontext import *\n",
    "from Models.helperfunctions import *\n",
    "from Models.PruningWithParent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from contextualbandits.online import PartitionedTS, ExploreFirst, _BasePolicy, _BasePolicyWithExploit, BootstrappedTS\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "X, y = parse_data(\"/Users/niekvandergaag/Documents/Thesis/Code/Datasets/Bibtex/Bibtex_data.txt\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "#X, y = select_top_k(X, y, k=15)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "nchoices = y.shape[1]\n",
    "base_algorithm = LogisticRegression(solver='lbfgs', warm_start=True, max_iter=500)\n",
    "base_algorithm2 = DecisionTreeClassifier(max_depth=8, min_samples_leaf=4, random_state=42)\n",
    "beta_prior = ((3./nchoices, 4), 2) # until there are at least 2 observations of each class, will use this prior\n",
    "beta_prior_ucb = ((5./nchoices, 4), 2) # UCB gives higher numbers, thus the higher positive prior\n",
    "beta_prior_ts = ((2./np.log2(nchoices), 4), 2)\n",
    "contexts = X # these are the features\n",
    "rewards = y # these are the labels\n",
    "n_arms = nchoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sparsity of the feature matrix (X)\n",
    "sparsity = 1.0 - (np.count_nonzero(X) / float(X.size))\n",
    "print(f\"Feature sparsity of Bibtex dataset: {sparsity * 100:.2f}%\")\n",
    "\n",
    "# Optionally log sparsity to understand its relationship with performance\n",
    "with open(\"feature_sparsity_log.txt\", \"a\") as log_file:\n",
    "    log_file.write(f\"Dataset: Bibtex, Sparsity: {sparsity * 100:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = X.shape[1] #number of features\n",
    "\n",
    "partitioned_ts_agent = PartitionedTS(\n",
    "    nchoices=nchoices,  \n",
    "    beta_prior=beta_prior_ts,  # beta prior for Thompson Sampling\n",
    "    random_state=4444,  # Set a random state for reproducibility\n",
    "    njobs=-1,  # Use all CPU cores for parallel processing\n",
    "    max_depth=6,  # Optional: You can pass additional decision tree parameters here\n",
    "    min_samples_leaf=4, # Optional: Minimum samples per leaf\n",
    "    ccp_alpha=0.00  # No pruning\n",
    ")\n",
    "\n",
    "TreeHeuristicWithPruning = PartitionedTSWithPruning(nchoices=nchoices, beta_prior=beta_prior_ts,\n",
    "                                                    random_state=6666,\n",
    "                                                    njobs=-1,\n",
    "                                                    max_depth=6,\n",
    "                                                    min_samples_leaf=4\n",
    "                                                    )\n",
    "\n",
    "# TreeHeuristicWithPruningParent = PartitionedTSWithPruningParent(nchoices=nchoices, beta_prior=beta_prior_ts,\n",
    "#                                                     random_state=6666,\n",
    "#                                                     njobs=-1,\n",
    "#                                                     max_depth=6,\n",
    "#                                                     min_samples_leaf=4,\n",
    "#                                                     #max_features=\"sqrt\"\n",
    "#                                                     )\n",
    "\n",
    "ts_nocontext_agent = ThompsonSamplingNoContext(nchoices=nchoices, beta_prior=(1,1))\n",
    "\n",
    "ete_agent = ExploreFirst(deepcopy(base_algorithm2), nchoices = nchoices,\n",
    "                             explore_rounds=1000, beta_prior=None, random_state = 8888)\n",
    "\n",
    "bootstrapped_ts_agent = BootstrappedTS(deepcopy(base_algorithm2), nchoices = nchoices,\n",
    "                                 beta_prior = beta_prior_ts, random_state = 2222)\n",
    "\n",
    "\n",
    "models = [partitioned_ts_agent, TreeHeuristicWithPruning,  ts_nocontext_agent, ete_agent, bootstrapped_ts_agent]\n",
    "#TreeHeuristicWithPruningParent,\n",
    "#xgb_agent, rf_agent, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lists will keep track of the rewards obtained by each policy\n",
    "rewards_part_ts, rewards_TreePruned, rewards_tsnocontext, rewards_ete, rewards_bootstrapped = [list() for i in range(len(models))]\n",
    "#\"\"\"rewards_xgb\"\"\", \"\"\"rewards_rf\"\"\", rewards_TreePP, \n",
    "\n",
    "lst_rewards = [rewards_part_ts, rewards_TreePruned, rewards_tsnocontext, rewards_ete, rewards_bootstrapped]\n",
    "#\"\"\"rewards_xgb\"\"\", \"\"\"rewards_rf\"\"\", rewards_TreePP,\n",
    "\n",
    "# batch size - algorithms will be refit after N rounds\n",
    "batch_size = 50\n",
    "# initial seed - all policies start with the same small random selection of actions/rewards\n",
    "first_batch = X[:batch_size, :]\n",
    "np.random.seed(1)\n",
    "action_chosen = np.random.randint(nchoices, size=batch_size)\n",
    "rewards_received = y[np.arange(batch_size), action_chosen]\n",
    "\n",
    "# fitting models for the first time\n",
    "for model in models:\n",
    "    if isinstance(model, ThompsonSamplingNoContext):\n",
    "        model.fit(None, actions=action_chosen, rewards=rewards_received)\n",
    "    else:\n",
    "        model.fit(X=first_batch, a=action_chosen, r=rewards_received)\n",
    "\n",
    "# these lists will keep track of which actions does each policy choose\n",
    "lst_a_pts, \\\n",
    "lst_a_THP, lst_a_tsnc, lst_a_ete, lst_a_bootstrapped, \\\n",
    " = [action_chosen.copy() for i in range(len(models))]\n",
    "#\"\"\"lst_a_xgb\"\"\", \"\"\"lst_a_rf\"\"\", lst_a_THPP,\n",
    "\n",
    "lst_actions = [lst_a_pts, lst_a_THP, lst_a_tsnc, lst_a_ete, lst_a_bootstrapped]\n",
    "#\"\"\"lst_a_xgb\"\"\", \"\"\"lst_a_rf\"\"\", lst_a_THPP,\n",
    "\n",
    "# rounds are simulated from the full dataset\n",
    "def simulate_rounds(model, rewards, actions_hist, X_global, y_global, batch_st, batch_end):\n",
    "    np.random.seed(batch_st)\n",
    "    \n",
    "    #print(X_global[batch_st:batch_end, :].shape )\n",
    "    ## choosing actions for this batch\n",
    "    if isinstance(model, ThompsonSamplingNoContext):\n",
    "        # Ensure to get an array of actions by calling pick_action for each sample in the batch\n",
    "        actions_this_batch = np.array([model.predict() for _ in range(batch_end - batch_st)])\n",
    "    else:\n",
    "        actions_this_batch = model.predict(X_global[batch_st:batch_end, :]).astype('uint8')\n",
    "\n",
    "    print(f\"Actions chosen this batch: {actions_this_batch}\")\n",
    "    # print(f\"Max action index: {actions_this_batch.max()}, nchoices: {nchoices}\")\n",
    "    \n",
    "    # keeping track of the sum of rewards received\n",
    "    rewards.append(y_global[np.arange(batch_st, batch_end), actions_this_batch].sum())\n",
    "    \n",
    "    # adding this batch to the history of selected actions\n",
    "    new_actions_hist = np.append(actions_hist, actions_this_batch)\n",
    "    \n",
    "    # now refitting the algorithms after observing these new rewards\n",
    "    np.random.seed(batch_st)\n",
    "    if isinstance(model, ThompsonSamplingNoContext):\n",
    "        for action, reward in zip(actions_this_batch, y_global[np.arange(batch_st, batch_end), actions_this_batch]):\n",
    "            model.update(action, reward)\n",
    "    else:\n",
    "        model.fit(X_global[:batch_end, :], new_actions_hist, y_global[np.arange(batch_end), new_actions_hist])\n",
    "    \n",
    "    return new_actions_hist\n",
    "\n",
    "# now running all the simulation\n",
    "for i in range(int(np.floor(X.shape[0] / batch_size))):\n",
    "    batch_st = (i + 1) * batch_size\n",
    "    batch_end = (i + 2) * batch_size\n",
    "    batch_end = np.min([batch_end, X.shape[0]])\n",
    "    \n",
    "    for model in range(len(models)):\n",
    "        lst_actions[model] = simulate_rounds(models[model],\n",
    "                                             lst_rewards[model],\n",
    "                                             lst_actions[model],\n",
    "                                             X, y,\n",
    "                                             batch_st, batch_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "def get_mean_reward(reward_lst, batch_size=batch_size):\n",
    "    mean_rew=list()\n",
    "    for r in range(len(reward_lst)):\n",
    "        mean_rew.append(sum(reward_lst[:r+1]) * 1.0 / ((r+1)*batch_size))\n",
    "    return mean_rew\n",
    "\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "lwd = 3\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors=plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(get_mean_reward(rewards_part_ts), label=\"Partitioned Thompson Sampling\",linewidth=lwd,color=colors[0])\n",
    "# plt.plot(get_mean_reward(rewards_part_ts1), label=\"Partitioned Thompson Sampling1\",linewidth=lwd,color=colors[14])\n",
    "# plt.plot(get_mean_reward(rewards_part_ts2), label=\"Partitioned Thompson Sampling2\",linewidth=lwd,color=colors[16])\n",
    "plt.plot(get_mean_reward(rewards_ete), label=\"Explore-Then-Exploit\",linewidth=lwd,color=colors[6])\n",
    "plt.plot(get_mean_reward(rewards_TreePruned), label=\"TreeHeuristicWithPruning\",linewidth=lwd, color=colors[2])\n",
    "#plt.plot(get_mean_reward(rewards_TreePP), label=\"TreeHeuristicWithPruningParent\", linewidth=lwd,color=colors[9])\n",
    "# plt.plot(get_mean_reward(rewards_rf), label=\"Bernoulli RF Agent\",linewidth=lwd,color=colors[4])\n",
    "# plt.plot(get_mean_reward(rewards_xgb), label=\"XGBoost Agent\",linewidth=lwd,color=colors[6])\n",
    "plt.plot(get_mean_reward(rewards_bootstrapped), label=\"Bootstrapped Thompson Sampling\",linewidth=lwd,color=colors[8])\n",
    "plt.plot(get_mean_reward(rewards_tsnocontext), label=\"Thompson Sampling No Context\",linewidth=lwd,color=colors[4])\n",
    "\n",
    "# legend\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 1.25])\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.15, 1.0),\n",
    "          fancybox=True, ncol=1, prop={'size':15})\n",
    "\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.xticks([i*20 for i in range(8)], [i*1000 for i in range(8)])\n",
    "\n",
    "\n",
    "plt.xlabel('Rounds (models updated every batch)', size=25)\n",
    "plt.ylabel('Cumulative Mean Reward', size=25)\n",
    "plt.title('Comparison of Contextual Bandit Policies\\nBibtex Dataset\\n(159 categories, 1836 attributes)',size=25)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "\n",
    "def parse_custom_svm(file_path, num_features=None):\n",
    "    \"\"\"\n",
    "    Custom parser for .svm files with multilabel format.\n",
    "    Parses features and multilabels explicitly and uses MultiLabelBinarizer for labels.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from {file_path}...\")\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Separate labels from features by splitting the line at the first space\n",
    "            parts = line.split(\" \", 1)\n",
    "            if len(parts) < 2:\n",
    "                print(f\"Warning: Skipping malformed line: {line}\")\n",
    "                continue\n",
    "\n",
    "            label_part, feature_part = parts\n",
    "            \n",
    "            # Process labels as multilabels\n",
    "            label_indices = [int(label) for label in label_part.split(\",\") if label.isdigit()]\n",
    "            labels.append(label_indices)\n",
    "\n",
    "            # Process features into a sparse dictionary format\n",
    "            feature_dict = {}\n",
    "            for feature in feature_part.split():\n",
    "                try:\n",
    "                    index, value = feature.split(\":\")\n",
    "                    feature_dict[int(index) - 1] = float(value)  # Indices in SVM format are 1-based\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Skipping malformed feature '{feature}' in line: {line}\")\n",
    "                    continue\n",
    "\n",
    "            features.append(feature_dict)\n",
    "\n",
    "    # Determine the number of features, if not specified\n",
    "    if num_features is None:\n",
    "        num_features = max(max(f.keys()) for f in features if f) + 1\n",
    "\n",
    "    # Convert features to a sparse matrix\n",
    "    X = dok_matrix((len(features), num_features), dtype=np.float32)\n",
    "    for i, feature_dict in enumerate(features):\n",
    "        for index, value in feature_dict.items():\n",
    "            X[i, index] = value\n",
    "\n",
    "    # Convert to CSR for efficient computation\n",
    "    X = X.tocsr()\n",
    "\n",
    "    # Binarize labels with MultiLabelBinarizer to handle multilabel outputs consistently\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(labels)\n",
    "\n",
    "    print(f\"Dataset loaded with shape: X={X.shape}, y={y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "# Define paths and feature counts for each dataset (optional)\n",
    "dataset_paths = {\n",
    "    'Mediamill': ('/Users/niekvandergaag/Documents/Thesis/Code/Datasets/mediamill/mediamill.svm', 120),\n",
    "    'Delicious': ('/Users/niekvandergaag/Documents/Thesis/Code/Datasets/delicious/delicious.svm', None),\n",
    "    'Bookmarks': ('/Users/niekvandergaag/Documents/Thesis/Code/Datasets/bookmarks/bookmarks.svm', None),\n",
    "    'EUR-Lex': ('/Users/niekvandergaag/Documents/Thesis/Code/Datasets/eurlex-directory-codes/eurlexdc.svm', None)\n",
    "}\n",
    "\n",
    "# Load each dataset\n",
    "datasets = {}\n",
    "for name, (path, num_features) in dataset_paths.items():\n",
    "    try:\n",
    "        X, y = parse_custom_svm(path, num_features)\n",
    "        datasets[name] = (X, y)\n",
    "        print(f\"{name} loaded with shape: X={X.shape}, y={y.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from contextualbandits.online import PartitionedTS, ExploreFirst, BootstrappedTS\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the models\n",
    "def initialize_models(nchoices):\n",
    "    #base_algorithm = LogisticRegression(solver='lbfgs', warm_start=True, max_iter=500)\n",
    "    base_algorithm2 = DecisionTreeClassifier(max_depth=8, min_samples_leaf=8)\n",
    "    beta_prior_ts = ((2./np.log2(nchoices), 4), 2)\n",
    "    \n",
    "    models = [\n",
    "        PartitionedTS(\n",
    "            nchoices=nchoices,\n",
    "            beta_prior=beta_prior_ts,\n",
    "            random_state=4444,\n",
    "            njobs=-1,\n",
    "            max_depth=8,\n",
    "            min_samples_leaf=8,\n",
    "            ccp_alpha=0.00\n",
    "        ),\n",
    "        PartitionedTSWithPruning(\n",
    "            nchoices=nchoices,\n",
    "            beta_prior=beta_prior_ts,\n",
    "            random_state=6666,\n",
    "            njobs=-1,\n",
    "            max_depth=8,\n",
    "            min_samples_leaf=8,\n",
    "            # ccp_alpha=0.001\n",
    "        ),\n",
    "        # PartitionedTSWithPruningParent(\n",
    "        #     nchoices=nchoices,\n",
    "        #     beta_prior=beta_prior_ts,\n",
    "        #     random_state=8888,\n",
    "        #     njobs=-1,\n",
    "        #     #max_depth=8,\n",
    "        #     min_samples_leaf=8,\n",
    "        #     # ccp_alpha=0.001\n",
    "        # ),\n",
    "        ThompsonSamplingNoContext(nchoices=nchoices, beta_prior=(1, 1)),\n",
    "\n",
    "        ExploreFirst(\n",
    "            deepcopy(base_algorithm2),\n",
    "            nchoices=nchoices,\n",
    "            explore_rounds=1000,\n",
    "            beta_prior=None,\n",
    "            random_state=8888\n",
    "        ),\n",
    "        BootstrappedTS(\n",
    "            deepcopy(base_algorithm2),\n",
    "            nchoices=nchoices,\n",
    "            beta_prior=beta_prior_ts,\n",
    "            random_state=2222\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Training simulation\n",
    "def simulate_training(models, X, y, batch_size=50):\n",
    "    nchoices = y.shape[1]\n",
    "    first_batch = X[:batch_size, :]\n",
    "    np.random.seed(1)\n",
    "    action_chosen = np.random.randint(nchoices, size=batch_size)\n",
    "    rewards_received = y[np.arange(batch_size), action_chosen]\n",
    "    \n",
    "    print(f\"Initializing models for the first batch of size {batch_size}...\")\n",
    "    \n",
    "    # Fit models for the first time\n",
    "    for model_idx, model in enumerate(models):\n",
    "        if isinstance(model, ThompsonSamplingNoContext):\n",
    "            model.fit(None, actions=action_chosen, rewards=rewards_received)\n",
    "        else:\n",
    "            model.fit(X=first_batch, a=action_chosen, r=rewards_received)\n",
    "        print(f\"Model {model_idx + 1}/{len(models)} initialized.\")\n",
    "    \n",
    "    # Initialize lists to track rewards and actions\n",
    "    lst_rewards = [[] for _ in models]\n",
    "    lst_actions = [action_chosen.copy() for _ in models]\n",
    "    \n",
    "    print(\"Starting batch simulations...\")\n",
    "    \n",
    "    # Simulate rounds\n",
    "    total_batches = int(np.floor(X.shape[0] / batch_size)) - 1\n",
    "    for i in range(total_batches):\n",
    "        batch_st = (i + 1) * batch_size\n",
    "        batch_end = (i + 2) * batch_size\n",
    "        batch_end = np.min([batch_end, X.shape[0]])\n",
    "        \n",
    "        for model_idx, model in enumerate(models):\n",
    "            np.random.seed(batch_st)\n",
    "            \n",
    "            if isinstance(model, ThompsonSamplingNoContext):\n",
    "                actions_this_batch = np.array([model.predict() for _ in range(batch_end - batch_st)])\n",
    "            else:\n",
    "                actions_this_batch = model.predict(X[batch_st:batch_end, :]).astype('uint8')\n",
    "            \n",
    "            # Collect rewards\n",
    "            lst_rewards[model_idx].append(y[np.arange(batch_st, batch_end), actions_this_batch].sum())\n",
    "            \n",
    "            # Update actions\n",
    "            lst_actions[model_idx] = np.append(lst_actions[model_idx], actions_this_batch)\n",
    "            \n",
    "            # Update models\n",
    "            if isinstance(model, ThompsonSamplingNoContext):\n",
    "                for action, reward in zip(actions_this_batch, y[np.arange(batch_st, batch_end), actions_this_batch]):\n",
    "                    model.update(action, reward)\n",
    "            else:\n",
    "                model.fit(X[:batch_end, :], lst_actions[model_idx], y[np.arange(batch_end), lst_actions[model_idx]])\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_batches:\n",
    "            print(f\"Completed batch {i + 1}/{total_batches}.\")\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return lst_rewards\n",
    "\n",
    "algorithm_colors = {\n",
    "    \"Partitioned Thompson Sampling\": \"#1f77b4\",  # Blue\n",
    "    \"TreeHeuristicWithPruning\": \"#ff7f0e\",       # Orange\n",
    "    \"Thompson Sampling No Context\": \"#2ca02c\",   # Green\n",
    "    \"Explore-Then-Exploit\": \"#d62728\",           # Red\n",
    "    \"Bootstrapped Thompson Sampling\": \"#9467bd\" # Purple\n",
    "}\n",
    "# Plotting results\n",
    "def plot_results(reward_lists, labels, title):\n",
    "    def get_mean_reward(reward_lst, batch_size):\n",
    "        mean_rew = [sum(reward_lst[:r + 1]) / ((r + 1) * batch_size) for r in range(len(reward_lst))]\n",
    "        return mean_rew\n",
    "    \n",
    "    batch_size = 50\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for idx, rewards in enumerate(reward_lists):\n",
    "        label = labels[idx]\n",
    "        color = algorithm_colors.get(label, f\"C{idx}\")\n",
    "        plt.plot(get_mean_reward(rewards, batch_size), label=label, linewidth=3, color=color)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "\n",
    "    plt.xlabel('Rounds (models updated every batch)', size=20)\n",
    "    plt.ylabel('Cumulative Mean Reward', size=20)\n",
    "    plt.title(title, size=20)\n",
    "    \n",
    "    plt.legend(loc='best', prop={'size': 14})\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Train models and plot results for each dataset\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    print(f\"\\nTraining on {dataset_name} dataset...\")\n",
    "    nchoices = y.shape[1]\n",
    "    models = initialize_models(nchoices)\n",
    "    rewards = simulate_training(models, X, y)\n",
    "    \n",
    "    model_labels = [\n",
    "        \"Partitioned Thompson Sampling\",\n",
    "        \"TreeHeuristicWithPruning\",\n",
    "        \"Thompson Sampling No Context\",\n",
    "        \"Explore-Then-Exploit\",\n",
    "        \"Bootstrapped Thompson Sampling\"\n",
    "    ]\n",
    "    \n",
    "    plot_title = f\"Comparison of Contextual Bandit Policies\\n{dataset_name} Dataset\\n({y.shape[1]} categories, {X.shape[1]} attributes)\"\n",
    "    plot_results(rewards, model_labels, plot_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openml\n",
    "from contextualbandits.evaluation import _check_random_state\n",
    "from contextualbandits.online import PartitionedTS, BootstrappedTS\n",
    "from contextualbandits.online import ExploreFirst\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample, shuffle\n",
    "from tqdm import trange\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the Models folder to sys.path\n",
    "sys.path.append('/Users/niekvandergaag/Documents/Thesis/Code')\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from Models.helperfunctions import *\n",
    "from Models.TSnocontext import *\n",
    "from Models.PruningWithParent import *\n",
    "\n",
    "# Create the plots directory if it doesn't exist\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "openml.config.apikey = \"\" #give valid APIkey\n",
    "openml.config.cache_directory = \"omlcache\"\n",
    "\n",
    "DATASET_IDS = [\n",
    "    2,5,7,\n",
    "    9,\n",
    "    10,\n",
    "    #11,\n",
    "    23,\n",
    "    26,\n",
    "    30,\n",
    "    34,\n",
    "    35,\n",
    "    39,\n",
    "    41,\n",
    "    42,\n",
    "    46,\n",
    "    57,\n",
    "    62,\n",
    "    #70,71,75,76,116,117,119,125,129,133,134,136,138,144,148,149,150,155,156,157,158,159,160, these are all synthetic data\n",
    "    163,171,180,181,182,183,184,185,\n",
    "    186,\n",
    "    187,\n",
    "    188,\n",
    "    244,\n",
    "    245,\n",
    "    248,\n",
    "    249,\n",
    "    253,\n",
    "    255,\n",
    "    259,\n",
    "    263,\n",
    "    265,\n",
    "    266,\n",
    "    272,\n",
    "    275,\n",
    "    276,\n",
    "    277,\n",
    "    278,\n",
    "    279,\n",
    "    285,\n",
    "    313,\n",
    "    327,\n",
    "    328,\n",
    "    329,\n",
    "    338,\n",
    "    340,\n",
    "    342,\n",
    "    343,\n",
    "    372,\n",
    "    373,\n",
    "    374,\n",
    "    375,\n",
    "    378,\n",
    "    379,\n",
    "    380,\n",
    "    381,\n",
    "    382,\n",
    "    383,\n",
    "    384,\n",
    "    385,\n",
    "    386,\n",
    "    387,\n",
    "    388,\n",
    "    389,\n",
    "    390,\n",
    "    391,\n",
    "    392,\n",
    "    393,\n",
    "    394,\n",
    "    395,\n",
    "    396,\n",
    "    397,\n",
    "    398,\n",
    "    399,\n",
    "    400,\n",
    "    401,\n",
    "    443,\n",
    "    449,\n",
    "    452,\n",
    "    453,\n",
    "    454,\n",
    "    455,\n",
    "    457,\n",
    "    458,\n",
    "    460,\n",
    "    462,\n",
    "    469,\n",
    "    473,\n",
    "    474,\n",
    "    477,\n",
    "    480,\n",
    "    488,\n",
    "    679,\n",
    "    694,\n",
    "    952,\n",
    "    1044,\n",
    "    1047,\n",
    "    1057,\n",
    "    1077,\n",
    "    1078,\n",
    "    1079,\n",
    "    1080,\n",
    "    1081,\n",
    "    1082,\n",
    "    1083,\n",
    "    1084,\n",
    "    1086,\n",
    "    1087,\n",
    "    1088,\n",
    "    1100,\n",
    "    1102,\n",
    "    1106,\n",
    "    # 1109,\n",
    "    # 1110,\n",
    "    # 1113,\n",
    "    # 1117,\n",
    "    # 1177,\n",
    "    # 1179,\n",
    "    # 1183,\n",
    "    # 1185,\n",
    "    # 1186,\n",
    "    # 1214,\n",
    "    # 1222,\n",
    "    # 1226,\n",
    "    # 1351,\n",
    "    # 1352,\n",
    "    # 1353,\n",
    "    # 1354,\n",
    "    # 1355,\n",
    "    # 1356,\n",
    "    # 1357,\n",
    "    # 1358,\n",
    "    # 1359,\n",
    "    # 1360,\n",
    "    # 1361,\n",
    "    # 1362,\n",
    "    # 1363,\n",
    "    # 1364,\n",
    "    # 1365,\n",
    "    # 1366,\n",
    "    # 1367,\n",
    "    # 1368,\n",
    "    # 1387,\n",
    "    # 1388,\n",
    "    # 1389,\n",
    "    # 1390,\n",
    "    # 1391,\n",
    "    # 1392,\n",
    "    # 1393,\n",
    "    # 1394,\n",
    "    # 1395,\n",
    "    # 1396,\n",
    "    # 1397,\n",
    "    # 1398,\n",
    "    # 1399,\n",
    "    # 1400,\n",
    "    # 1401,\n",
    "    # 1402,\n",
    "    # 1403,\n",
    "    # 1404,\n",
    "    # 1405,\n",
    "    # 1406,\n",
    "    # 1407,\n",
    "    # 1408,\n",
    "    # 1409,\n",
    "    # 1410,\n",
    "    # 1459,\n",
    "    # 1465,\n",
    "    # 1466,\n",
    "    # 1472,\n",
    "    # 1475,\n",
    "    # 1476,\n",
    "    # 1477,\n",
    "    # 1478,\n",
    "    # 1481,\n",
    "    # 1482,\n",
    "    # 1483,\n",
    "    # 1497,\n",
    "    # 1508,\n",
    "    # 1509,\n",
    "    # 1512,\n",
    "    # 1513,\n",
    "    # 1515,\n",
    "    # 1516,\n",
    "    # 1517,\n",
    "    # 1518,\n",
    "    # 1519,\n",
    "    # 1520,\n",
    "    # 1523,\n",
    "    # 1525,\n",
    "    # 1526,\n",
    "    # 1527,\n",
    "    # 1528,\n",
    "    # 1529,\n",
    "    # 1530,\n",
    "    # 1531,\n",
    "    # 1532,\n",
    "    # 1533,\n",
    "    # 1534,\n",
    "    # 1535,\n",
    "    # 1536,\n",
    "    # 1537,\n",
    "    # 1538,\n",
    "    # 1539,\n",
    "    # 1540,\n",
    "    # 1541,\n",
    "    # 1542,\n",
    "    # 1543,\n",
    "    # 1544,\n",
    "    # 1545,\n",
    "    # 1546,\n",
    "    # 1548,\n",
    "    # 1549,\n",
    "    # 1551,\n",
    "    # 1552,\n",
    "    # 1554,\n",
    "    # 1555,\n",
    "    # 1559,\n",
    "    # 1560,\n",
    "    # 1565,\n",
    "    # 1567,\n",
    "    # 1568,\n",
    "    # 1569,\n",
    "    # 1596,\n",
    "    # 4340,\n",
    "    # 4538,\n",
    "    # 4541,\n",
    "    # 4552,\n",
    "]\n",
    "\n",
    "\n",
    "def get_multiclass_dataset(dataset_id):\n",
    "    \"\"\"\n",
    "    Retrieve a dataset and transform it so it can be used with the contextual bandit algorithms.\n",
    "    Handles missing values and categorical data, including boolean data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(\n",
    "            dataset_id=dataset_id,\n",
    "            download_data=False,\n",
    "            download_qualities=False,\n",
    "            download_features_meta_data=False,\n",
    "        )\n",
    "        print(f\"\\nProcessing {dataset.name}\")\n",
    "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process dataset {dataset_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Separate numeric and categorical features\n",
    "    X_numeric = X.select_dtypes(include=[np.number])\n",
    "    X_categorical = X.select_dtypes(exclude=[np.number])\n",
    "\n",
    "    # Handle boolean columns (convert them to object/string type)\n",
    "    bool_columns = X_categorical.select_dtypes(include=[bool]).columns\n",
    "    X_categorical[bool_columns] = X_categorical[bool_columns].astype(str)\n",
    "\n",
    "    # Impute missing values for numeric features (if they exist)\n",
    "    if not X_numeric.empty:\n",
    "        imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        X_numeric_imputed = imputer_numeric.fit_transform(X_numeric)\n",
    "    else:\n",
    "        X_numeric_imputed = np.empty((X.shape[0], 0))  # No numeric data, create an empty array with correct row shape\n",
    "\n",
    "    # Impute missing values for categorical features (if they exist)\n",
    "    if not X_categorical.empty:\n",
    "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "        X_categorical_imputed = imputer_categorical.fit_transform(X_categorical)\n",
    "\n",
    "        # Ensure that the DataFrame has the right number of columns\n",
    "        if X_categorical_imputed.shape[1] != len(X_categorical.columns):\n",
    "            print(f\"Warning: Shape mismatch after imputation. Adjusting column count.\")\n",
    "            X_categorical_imputed_df = pd.DataFrame(X_categorical_imputed, columns=X_categorical.columns[:X_categorical_imputed.shape[1]])\n",
    "        else:\n",
    "            X_categorical_imputed_df = pd.DataFrame(X_categorical_imputed, columns=X_categorical.columns)\n",
    "\n",
    "        # OneHotEncode categorical features\n",
    "        X_categorical_encoded = pd.get_dummies(X_categorical_imputed_df).to_numpy()\n",
    "    else:\n",
    "        X_categorical_encoded = np.empty((X.shape[0], 0))  # No categorical data, create an empty array with correct row shape\n",
    "\n",
    "    # Ensure both arrays are 2D before concatenating\n",
    "    if X_numeric_imputed.ndim == 1:\n",
    "        X_numeric_imputed = X_numeric_imputed.reshape(-1, 1)\n",
    "    if X_categorical_encoded.ndim == 1:\n",
    "        X_categorical_encoded = X_categorical_encoded.reshape(-1, 1)\n",
    "\n",
    "    # If we still encounter a dimensional mismatch, skip this dataset\n",
    "    try:\n",
    "        # Concatenate numeric and encoded categorical features\n",
    "        X_imputed = np.hstack([X_numeric_imputed, X_categorical_encoded])\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping dataset {dataset.name} due to dimension mismatch: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Ensure that all target values are strings to avoid issues during label encoding\n",
    "    y = y.astype(str)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_imputed = scaler.fit_transform(X_imputed)\n",
    "    # Encode target variable\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    try:\n",
    "        y = lb.fit_transform(y)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping dataset {dataset.name} due to label transformation error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # If the target has only one class, create a binary classification target\n",
    "    if y.shape[1] == 1:\n",
    "        y = np.concatenate([1 - y, y], axis=1)\n",
    "\n",
    "    return shuffle(X_imputed, y)\n",
    "\n",
    "\n",
    "\n",
    "def evaluateFullyLabeled(\n",
    "    policy, X, y_onehot, online=False, shuffle=True, update_freq=50, random_state=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a policy on fully-labeled data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : obj\n",
    "        Policy to be evaluated (already fitted to data). Must have a 'predict' method.\n",
    "        If it is an online policy, it must also have a 'fit' method.\n",
    "    X : array (n_samples, n_features)\n",
    "        Covariates for each observation.\n",
    "    y_onehot : array (n_samples, n_arms)\n",
    "        Labels (zero or one) for each class for each observation.\n",
    "    online : bool\n",
    "        Whether the algorithm should be fit to batches of data with a 'partial_fit' method,\n",
    "        or to all historical data each time.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the data (X and y_onehot) before passing through it.\n",
    "        Be aware that data is shuffled in-place.\n",
    "    update_freq : int\n",
    "        Batch size - how many observations to predict before refitting the model.\n",
    "    random_state : int, None, RandomState, or Generator\n",
    "        Either an integer which will be used as seed for initializing a\n",
    "        ``Generator`` object for random number generation, a ``RandomState``\n",
    "        object (from NumPy) from which to draw an integer, or a ``Generator``\n",
    "        object (from NumPy), which will be used directly.\n",
    "        This is used when shuffling and when selecting actions at random for\n",
    "        the first batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_rew : array (n_samples,)\n",
    "        Mean reward obtained at each batch.\n",
    "    \"\"\"\n",
    "    if type(X).__name__ == \"DataFrame\":\n",
    "        X = X.as_matrix()\n",
    "    if type(y_onehot).__name__ == \"DataFrame\":\n",
    "        y_onehot = y_onehot.as_matrix()\n",
    "\n",
    "    assert type(X).__name__ == \"ndarray\"\n",
    "    assert type(y_onehot).__name__ == \"ndarray\"\n",
    "    assert isinstance(online, bool)\n",
    "    assert isinstance(shuffle, bool)\n",
    "    assert isinstance(update_freq, int)\n",
    "    assert X.shape[0] > update_freq\n",
    "    assert X.shape[0] == y_onehot.shape[0]\n",
    "    assert X.shape[0] > 0\n",
    "\n",
    "    rs = _check_random_state(random_state)\n",
    "\n",
    "    if shuffle:\n",
    "        new_order = np.arange(X.shape[0])\n",
    "        rs.shuffle(new_order)\n",
    "        X = X[new_order, :]\n",
    "        y_onehot = y_onehot[new_order, :]\n",
    "\n",
    "    rewards_per_turn = list()\n",
    "    history_actions = np.array([], dtype=int)\n",
    "    history_propensities = np.array([], dtype=np.float64)\n",
    "    n_choices = y_onehot.shape[1]\n",
    "\n",
    "    ## initial seed\n",
    "    batch_features = X[:update_freq, :]\n",
    "    batch_actions = rs.integers(y_onehot.shape[1], size=update_freq)\n",
    "    batch_rewards = y_onehot[np.arange(update_freq), batch_actions]\n",
    "    batch_propensities = np.full(update_freq, 1 / n_choices)\n",
    "\n",
    "    if online:\n",
    "        policy.partial_fit(batch_features, batch_actions, batch_rewards)\n",
    "    else:\n",
    "        policy.fit(batch_features, batch_actions, batch_rewards)\n",
    "\n",
    "    history_actions = np.append(history_actions, batch_actions)\n",
    "\n",
    "    ## running the loop\n",
    "    for i in range(1, int(np.floor(X.shape[0] / update_freq))):\n",
    "        st = (i) * update_freq\n",
    "        end = np.min([(i + 1) * update_freq, X.shape[0]])  # Ensure we don't go beyond the dataset size\n",
    "        # end = (i + 1) * update_freq\n",
    "        # end = np.min([end, X.shape[0]])\n",
    "\n",
    "        batch_features = X[st:end, :]\n",
    "        batch_actions = policy.predict(batch_features)\n",
    "        # Check for correct shape before indexing\n",
    "        assert len(batch_actions) == (end - st), f\"Shape mismatch: {len(batch_actions)} vs {(end - st)}\"\n",
    "\n",
    "        batch_rewards = y_onehot[np.arange(st, end), batch_actions]\n",
    "\n",
    "        # print(f\"Batch features shape: {batch_features.shape}\")\n",
    "        # print(f\"Batch actions shape: {batch_actions.shape}\")\n",
    "        # print(f\"y_onehot shape: {y_onehot.shape}\")\n",
    "        #print(f\"st: {st}, end: {end}\")\n",
    "\n",
    "\n",
    "        rewards_per_turn.append(batch_rewards.sum())\n",
    "\n",
    "        # break if we've reached the end of the data'\n",
    "        # TODO: commit this change upstream\n",
    "        if end == X.shape[0]:\n",
    "            # stop before fitting the models again\n",
    "            break\n",
    "\n",
    "        if online:\n",
    "            policy.partial_fit(batch_features, batch_actions, batch_rewards)\n",
    "        else:\n",
    "            history_actions = np.append(history_actions, batch_actions)\n",
    "            policy.fit(\n",
    "                X[:end, :], history_actions, y_onehot[np.arange(end), history_actions]\n",
    "            )\n",
    "\n",
    "    ## outputting results\n",
    "    def get_mean_reward(reward_lst, batch_size):\n",
    "        mean_rew = list()\n",
    "        for r in range(len(reward_lst)):\n",
    "            mean_rew.append(sum(reward_lst[: r + 1]) / ((r + 1) * batch_size))\n",
    "        return mean_rew\n",
    "\n",
    "    def get_cumulative_reward(reward_lst):\n",
    "        cum_rew = list()\n",
    "        for r in range(len(reward_lst)):\n",
    "            cum_rew.append(sum(reward_lst[: r + 1]))\n",
    "        return cum_rew\n",
    "\n",
    "    return np.array(get_mean_reward(rewards_per_turn, update_freq))\n",
    "\n",
    "\n",
    "def plot_rewards(mean_rewards, ax=None, label=None, std=False):\n",
    "    \"\"\"\n",
    "    Plot the cumulative rewards\n",
    "    :param mean_rewards: the mean rewards\n",
    "    :param ax: the axis to plot on\n",
    "    :param label: the label for the plot\n",
    "    :param std: whether to plot the standard deviation\n",
    "    \"\"\"\n",
    "    assert ax is not None\n",
    "    cumulative_rewards = np.cumsum(mean_rewards, axis=1)\n",
    "    ax.plot(np.mean(cumulative_rewards, axis=0), label=label)\n",
    "    if std:\n",
    "        ax.fill_between(\n",
    "            np.arange(cumulative_rewards.shape[1]),\n",
    "            np.mean(cumulative_rewards, axis=0) - np.std(cumulative_rewards, axis=0),\n",
    "            np.mean(cumulative_rewards, axis=0) + np.std(cumulative_rewards, axis=0),\n",
    "            alpha=0.1,\n",
    "        )\n",
    "\n",
    "def select_top_k(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select the top-k most frequent labels.\n",
    "    \"\"\"\n",
    "    label_counts = y.sum(axis=0)\n",
    "    top_k_labels = np.argsort(label_counts)[::-1][:k]\n",
    "    y = y[:, top_k_labels]\n",
    "    nonzero_indices = ~np.all(y == 0, axis=1)\n",
    "    return X[nonzero_indices], y[nonzero_indices]\n",
    "\n",
    "\n",
    "def reduce_success_rate(X, y, rate=0.1):\n",
    "    \"\"\"\n",
    "    Reduce the success rates of the arms\n",
    "    \"\"\"\n",
    "    n_negative_samples = int(X.shape[0] * (1 - rate) / rate)\n",
    "    X_zero = resample(X, n_samples=n_negative_samples, random_state=0)\n",
    "    y_zero = np.zeros((n_negative_samples, y.shape[1]))\n",
    "\n",
    "    return shuffle(np.vstack([X, X_zero]), np.vstack([y, y_zero]))\n",
    "\n",
    "\n",
    "def test_reduce_success_rate():\n",
    "    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "    y = np.array([[1, 0], [0, 1], [1, 0], [0, 1], [1, 0]])\n",
    "    X, y = reduce_success_rate(X, y, 0.01)\n",
    "    assert y.sum() == 5\n",
    "    assert X.shape[0] == 500\n",
    "    assert y.shape[0] == 500\n",
    "\n",
    "\n",
    "def evaluate_policies(policies, X, y, runs=5, update_freq=50):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    for policy_class, policy_kwargs in policies:\n",
    "        mean_rewards = []\n",
    "        for random_state in trange(1, runs + 1):\n",
    "            policy = policy_class(\n",
    "                nchoices=y.shape[1], **policy_kwargs, random_state=random_state\n",
    "            )\n",
    "            mean_reward = evaluateFullyLabeled(\n",
    "                policy,\n",
    "                X,\n",
    "                y,\n",
    "                online=False,\n",
    "                shuffle=True,\n",
    "                update_freq=update_freq,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            mean_rewards.append(mean_reward)\n",
    "\n",
    "        plot_rewards(mean_rewards, ax=ax, label=policy_class.__name__, std=True)\n",
    "\n",
    "    ax.legend()\n",
    "    return fig\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset_id in DATASET_IDS:\n",
    "        # retrieve the covertype dataset\n",
    "        X, y = get_multiclass_dataset(dataset_id)\n",
    "        if X is None or y is None:\n",
    "            print(f\"Skipping dataset {dataset_id} due to retrieval error.\")\n",
    "            continue\n",
    "\n",
    "        # select the top 5 arms\n",
    "        X, y = select_top_k(X, y, 5)\n",
    "\n",
    "        # reduce the success rate of the arms\n",
    "        X, y = reduce_success_rate(X, y, 0.1)\n",
    "\n",
    "        print(\n",
    "            f\"Conversion rates for the {y.shape[1]} actions: {np.round(np.mean(y, axis=0), 4)}\"\n",
    "        )\n",
    "\n",
    "        # reduce the size of the dataset\n",
    "        if len(y) > 10_000:\n",
    "            X, y = shuffle(X, y, n_samples=10_000, random_state=0)\n",
    "\n",
    "        fig = evaluate_policies(\n",
    "            [\n",
    "                # (PartitionedTSWithPruningParent, {\"min_samples_leaf\": 10}),\n",
    "                (PartitionedTSWithPruning, {\"min_samples_leaf\": 10}),\n",
    "                (PartitionedTS, {\"min_samples_leaf\": 10}),\n",
    "                (ThompsonSamplingNoContext, {}),\n",
    "            ],\n",
    "            X,\n",
    "            y,\n",
    "            runs=5,\n",
    "            update_freq=100\n",
    "        )\n",
    "        \n",
    "        # save the figure\n",
    "        fig.savefig(f\"plots/{dataset_id}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_policies_and_save(policies, X, y, dataset_id, runs=5, update_freq=50):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    all_mean_rewards = []\n",
    "\n",
    "    for policy_class, policy_kwargs in policies:\n",
    "        mean_rewards = []\n",
    "        for random_state in trange(1, runs + 1):\n",
    "            policy = policy_class(\n",
    "                nchoices=y.shape[1], **policy_kwargs, random_state=random_state\n",
    "            )\n",
    "            mean_reward = evaluateFullyLabeled(\n",
    "                policy,\n",
    "                X,\n",
    "                y,\n",
    "                online=False,\n",
    "                shuffle=True,\n",
    "                update_freq=update_freq,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            mean_rewards.append(mean_reward)\n",
    "\n",
    "        # Save mean rewards for the policy and dataset\n",
    "        policy_name = policy_class.__name__\n",
    "        np.savetxt(f\"plots/{dataset_id}_{policy_name}_mean_rewards.csv\", mean_rewards, delimiter=\",\")\n",
    "\n",
    "        all_mean_rewards.append(mean_rewards)\n",
    "        plot_rewards(mean_rewards, ax=ax, label=policy_class.__name__, std=True)\n",
    "\n",
    "    ax.legend()\n",
    "    return fig, all_mean_rewards\n",
    "\n",
    "# Example usage in main loop\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset_id in DATASET_IDS:\n",
    "        # retrieve the covertype dataset\n",
    "        X, y = get_multiclass_dataset(dataset_id)\n",
    "        if X is None or y is None:\n",
    "            print(f\"Skipping dataset {dataset_id} due to retrieval error.\")\n",
    "            continue\n",
    "\n",
    "        # select the top 5 arms\n",
    "        X, y = select_top_k(X, y, 5)\n",
    "\n",
    "        # reduce the success rate of the arms\n",
    "        X, y = reduce_success_rate(X, y, 0.1)\n",
    "\n",
    "        print(\n",
    "            f\"Conversion rates for the {y.shape[1]} actions: {np.round(np.mean(y, axis=0), 4)}\"\n",
    "        )\n",
    "\n",
    "        # reduce the size of the dataset\n",
    "        if len(y) > 10_000:\n",
    "            X, y = shuffle(X, y, n_samples=10_000, random_state=0)\n",
    "\n",
    "        fig, _ = evaluate_policies_and_save(\n",
    "            [\n",
    "                # (PartitionedTSWithPruningParent, {\"min_samples_leaf\": 10}),\n",
    "                (PartitionedTSWithPruning, {\"min_samples_leaf\": 10}),\n",
    "                (PartitionedTS, {\"min_samples_leaf\": 10}),\n",
    "                (ThompsonSamplingNoContext, {}),\n",
    "            ],\n",
    "            X,\n",
    "            y,\n",
    "            dataset_id=dataset_id,\n",
    "            runs=5,\n",
    "            update_freq=100\n",
    "        )\n",
    "\n",
    "        # save the figure\n",
    "        fig.savefig(f\"plots/{dataset_id}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_rewards(dataset_ids, policy_name):\n",
    "    \"\"\"\n",
    "    Load rewards from CSV files for each dataset and policy.\n",
    "    Handle different array lengths by padding with NaNs or truncating.\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    max_length = 0\n",
    "\n",
    "    # First, find the maximum length of the rewards arrays\n",
    "    for dataset_id in dataset_ids:\n",
    "        filename = f\"plots/{dataset_id}_{policy_name}_mean_rewards.csv\"\n",
    "        if os.path.exists(filename):\n",
    "            rewards = np.loadtxt(filename, delimiter=\",\")\n",
    "            if rewards.size > 0:  # Only process non-empty arrays\n",
    "                if rewards.ndim == 1:\n",
    "                    rewards = rewards.reshape(1, -1)  # Convert 1D arrays to 2D\n",
    "                max_length = max(max_length, rewards.shape[1])  # Track max length\n",
    "        else:\n",
    "            print(f\"Warning: File {filename} not found.\")\n",
    "\n",
    "    # Load the rewards and pad/truncate them to the same length\n",
    "    for dataset_id in dataset_ids:\n",
    "        filename = f\"plots/{dataset_id}_{policy_name}_mean_rewards.csv\"\n",
    "        if os.path.exists(filename):\n",
    "            rewards = np.loadtxt(filename, delimiter=\",\")\n",
    "            if rewards.size > 0:  # Only process non-empty arrays\n",
    "                if rewards.ndim == 1:\n",
    "                    rewards = rewards.reshape(1, -1)  # Convert 1D arrays to 2D\n",
    "                if rewards.shape[1] < max_length:\n",
    "                    # Pad with NaNs to make the lengths equal\n",
    "                    pad_width = max_length - rewards.shape[1]\n",
    "                    rewards = np.pad(rewards, ((0, 0), (0, pad_width)), constant_values=np.nan)\n",
    "                elif rewards.shape[1] > max_length:\n",
    "                    # Truncate to the maximum length\n",
    "                    rewards = rewards[:, :max_length]\n",
    "\n",
    "                all_rewards.append(rewards)\n",
    "            else:\n",
    "                print(f\"Skipping {filename} because it's empty or invalid.\")\n",
    "        else:\n",
    "            print(f\"Warning: File {filename} not found.\")\n",
    "\n",
    "    if len(all_rewards) == 0:\n",
    "        print(f\"No valid rewards found for policy {policy_name}.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Loaded rewards for {policy_name}: {len(all_rewards)} datasets processed.\")\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "def plot_aggregated_rewards(dataset_ids, policies):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    for policy_name in policies:\n",
    "        all_rewards = load_rewards(dataset_ids, policy_name)\n",
    "\n",
    "        if not all_rewards:  # Skip if no rewards are loaded\n",
    "            continue\n",
    "\n",
    "        # Compute mean and standard deviation across datasets, ignoring NaNs\n",
    "        mean_rewards_list = []\n",
    "        std_rewards_list = []\n",
    "\n",
    "        for rewards in all_rewards:\n",
    "            # Compute cumulative rewards\n",
    "            cumulative_rewards = np.nancumsum(rewards, axis=1)\n",
    "            mean_rewards_list.append(np.nanmean(cumulative_rewards, axis=0))\n",
    "            std_rewards_list.append(np.nanstd(cumulative_rewards, axis=0))\n",
    "\n",
    "        if mean_rewards_list:\n",
    "            mean_rewards = np.nanmean(mean_rewards_list, axis=0)\n",
    "            std_rewards = np.nanmean(std_rewards_list, axis=0)\n",
    "\n",
    "            # Plot the aggregated results\n",
    "            ax.plot(mean_rewards, label=policy_name)\n",
    "            ax.fill_between(\n",
    "                np.arange(len(mean_rewards)),\n",
    "                mean_rewards - std_rewards,\n",
    "                mean_rewards + std_rewards,\n",
    "                alpha=0,\n",
    "            )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title('Comparison of Contextual Bandit Policies\\n97 OpenML Datasets')\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Cumulative Reward')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    policies = [ \"PartitionedTSWithPruning\", \"PartitionedTS\", \"ThompsonSamplingNoContext\"]\n",
    "    plot_aggregated_rewards(DATASET_IDS, policies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
